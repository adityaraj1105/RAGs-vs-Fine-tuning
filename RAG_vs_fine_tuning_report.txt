Project Report: Comparative Analysis of RAG and Fine-Tuned GPT-2 on SQuAD Dataset

1. Introduction
This project aims to evaluate and compare the performance of two different models for question answering: a Retrieval-Augmented Generation (RAG) model and a fine-tuned GPT-2 model. The comparison focuses on key performance metrics such as F1-score and accuracy using the SQuAD dataset.

The models' evaluation is based on:

RAG (using DistilBART): A hybrid model that augments generative transformers with retrieval mechanisms.
Fine-tuned GPT-2: A standard generative model fine-tuned for the task of question-answering.

2. Dataset
I used the SQuAD (Stanford Question Answering Dataset) for evaluating both models. This dataset consists of questions posed on a set of Wikipedia articles, where the model has to generate an answer based on the article content.

Dataset Statistics:
Training set size: 87,599 examples
Validation set size: 10,570 examples

3. Models and Techniques
3.1 RAG Model
The RAG (Retrieval-Augmented Generation) model combines retrieval-based techniques with generative transformers. For the experiment, I utilized a pre-trained DistilBART as the base model and fine-tuned it on the SQuAD dataset.

3.2 Fine-tuned GPT-2
The GPT-2 model was fine-tuned on the SQuAD dataset for the task of question answering. GPT-2 is a pure generative model, lacking the retrieval mechanism that RAG uses, which makes this comparison valuable in understanding the impact of retrieval on QA performance.

4. Evaluation Metrics
The following metrics were used to evaluate the models:

F1-Score: A measure that considers both precision and recall to compute the accuracy of the model's answers.
Accuracy: The percentage of correct answers compared to the ground truth answers.

5. Results
The comparison of the two models is based on the accuracy and F1-score obtained from the test data.

Model	Accuracy
RAG	62.45%
GPT-2 (Fine-tuned)	43.84%
5.1 Observations
RAG Model Performance: The RAG model outperforms the fine-tuned GPT-2 model in terms of accuracy, achieving a score of 62.45%, which reflects the benefits of incorporating a retrieval component that allows the model to look up relevant information.

GPT-2 Performance: The fine-tuned GPT-2 model, while effective as a generative model, achieved an accuracy of 43.84%. Its lower performance indicates its limitation in handling knowledge recall compared to RAG.

6. Conclusion
This comparative analysis highlights that Retrieval-Augmented Generation (RAG), with its retrieval mechanism, provides a significant advantage in question-answering tasks when compared to a fine-tuned GPT-2 model. The improvement in accuracy by 18.61% demonstrates the effectiveness of combining retrieval techniques with generative models in domains that require knowledge-based reasoning, such as question answering.

In future work, exploring further fine-tuning strategies and augmenting the GPT-2 model with external knowledge sources could bridge the performance gap between RAG and GPT-2. Additionally, experimenting with other variations and combinations of Retrieval-Augmented Generation (RAG) frameworks and large language models (LLMs), such as GPT-2 Large, could yield more comprehensive insights and potentially enhance overall model performance.

